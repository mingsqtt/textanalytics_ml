{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> table {float:left} </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> table {float:left} </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: tqdm in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (4.40.0)\n",
      "Requirement already satisfied: lazyme in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (0.0.23)\n",
      "Requirement already satisfied: nltk in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: gensim in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from torch) (1.17.4)\n",
      "Requirement already satisfied: six in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from nltk) (1.13.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from gensim) (1.3.3)\n",
      "Requirement already satisfied: boto3 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.10.30)\n",
      "Requirement already satisfied: boto>=2.32 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.30 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.30)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from botocore<1.14.0,>=1.13.30->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in d:\\apps\\anaconda3\\envs\\torch-nlp\\lib\\site-packages (from botocore<1.14.0,>=1.13.30->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\Anaconda3\\envs\\torch-nlp\\lib\\runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Z370\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install torch tqdm lazyme nltk gensim\n",
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "    print(\"OK\")\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Toxic Comments\n",
    "\n",
    "Lets apply what we learnt in a realistic task and **fight cyber-abuse with NLP**!\n",
    "\n",
    "From https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/\n",
    "\n",
    "> *The threat of abuse and harassment online means that many people stop <br>*\n",
    "> *expressing themselves and give up on seeking different opinions. <br>*\n",
    "> *Platforms struggle to effectively facilitate conversations, leading many <br>*\n",
    "> *communities to limit or completely shut down user comments.*\n",
    "\n",
    "\n",
    "The goal of the task is to build a model to detect different types of of toxicity:\n",
    "\n",
    " - toxic\n",
    " - severe toxic\n",
    " - threats\n",
    " - obscenity\n",
    " - insults\n",
    " - identity-based hate\n",
    " \n",
    "In this part, you'll be munging the data as how I would be doing it at work. \n",
    "\n",
    "Your task is to train a feed-forward network on the toxic comments given the skills we have accomplished thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging into the data...\n",
    "\n",
    "If you're using linux/Mac you can use these bang commands in the notebook:\n",
    "\n",
    "```\n",
    "!pip3 install kaggle\n",
    "!mkdir -p /content/.kaggle/\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > /content/.kaggle/kaggle.json\n",
    "!chmod 600 /content/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
    "!unzip /content/.kaggle/competitions/jigsaw-toxic-comment-classification-challenge/*\n",
    "```\n",
    "\n",
    "Otherwise, download the data from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\projects\\\\tsundoku-master\\\\data\\\\toxic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"D:/projects/tsundoku-master/data/toxic/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['comment_text_tokenzied'] = df_train['comment_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Explanation, Why, the, edits, made, under, my...\n",
       "1    [D'aww, !, He, matches, this, background, colo...\n",
       "2    [Hey, man, ,, I, 'm, really, not, trying, to, ...\n",
       "3    [``, More, I, ca, n't, make, any, real, sugges...\n",
       "4    [You, ,, sir, ,, are, my, hero, ., Any, chance...\n",
       "Name: comment_text_tokenzied, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['comment_text_tokenzied'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Just in case your Jupyter kernel dies, save the tokenized text =)\n",
    "\n",
    "# To save your tokenized text you can do this:\n",
    "import pickle\n",
    "with open('train_tokenized_text.pkl', 'wb') as fout:\n",
    "    pickle.dump(df_train['comment_text_tokenzied'], fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Explanation, Why, the, edits, made, under, my...\n",
       "1    [D'aww, !, He, matches, this, background, colo...\n",
       "2    [Hey, man, ,, I, 'm, really, not, trying, to, ...\n",
       "3    [``, More, I, ca, n't, make, any, real, sugges...\n",
       "4    [You, ,, sir, ,, are, my, hero, ., Any, chance...\n",
       "Name: comment_text_tokenzied, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load it back:\n",
    "import pickle\n",
    "with open('train_tokenized_text.pkl', 'rb') as fin:\n",
    "    text_tokenzied = pickle.load(fin)\n",
    "    df_train['comment_text_tokenzied'] = text_tokenzied\n",
    "text_tokenzied[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get a one-hot?\n",
    "\n",
    "There are many variants of how to get your one-hot embeddings from the individual columns.\n",
    "\n",
    "This is one way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column_names = \"toxic\tsevere_toxic\tobscene\tthreat\tinsult\tidentity_hate\".split()\n",
    "y_train = df_train[label_column_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_y_train = torch.tensor(y_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot to indices of the column.\n",
    "\n",
    "print(np.argmax(df_train[label_column_names].values, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.sents = tokenized_texts\n",
    "        self.labels = labels\n",
    "        self.vocab = Dictionary(tokenized_texts)\n",
    "        special_tokens = {'<pad>': 0, '<unk>':1}\n",
    "        self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        self.max_len = max([len(sent) for sent in tokenized_texts])\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self._len = len(tokenized_texts)\n",
    "        \n",
    "    def __getitem__(self, sent_index):\n",
    "        sent = self.sents[sent_index]\n",
    "        vectorized_sent = self.vectorize(sent)\n",
    "        sent_len = len(vectorized_sent)\n",
    "        pad_len = self.max_len - len(vectorized_sent)\n",
    "        pad_dim = (0, pad_len)\n",
    "        padded_vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "        return {'x':padded_vectorized_sent, \n",
    "                'y':torch.tensor(self.labels[sent_index]), \n",
    "                'x_len':sent_len}\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        return torch.tensor(self.vocab.doc2idx(tokens))\n",
    "        \n",
    "    def unvectorize(self, indices):\n",
    "        return [self.vocab[i] for i in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_column_names = \"toxic\tsevere_toxic\tobscene\tthreat\tinsult\tidentity_hate\".split()\n",
    "toxic_data = ToxicDataset(text_tokenzied, df_train[label_column_names].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[   103,   1128,     11,  ...,      0,      0,      0],\n",
      "        [   103,   6134,     90,  ...,      0,      0,      0],\n",
      "        [   103,   3914,    409,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  4705,    870,    104,  ...,      0,      0,      0],\n",
      "        [  2062,    165,     82,  ...,      0,      0,      0],\n",
      "        [ 16971,      2, 116462,  ...,      0,      0,      0]]), 'y': tensor([[0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0]]), 'x_len': tensor([977, 834, 405, 351, 304, 229, 214, 211, 180, 161, 161, 146, 134, 131,\n",
      "        109, 102,  96,  96,  92,  92,  86,  85,  84,  84,  81,  79,  78,  77,\n",
      "         76,  76,  73,  69,  68,  67,  64,  63,  61,  60,  57,  54,  53,  52,\n",
      "         51,  51,  51,  50,  48,  48,  47,  45,  43,  42,  42,  40,  37,  36,\n",
      "         35,  35,  35,  32,  32,  32,  31,  31,  30,  30,  27,  26,  26,  25,\n",
      "         25,  25,  24,  23,  22,  22,  22,  22,  21,  20,  19,  19,  18,  17,\n",
      "         16,  15,  14,  14,  13,  13,  13,  12,  11,  10,   9,   8,   8,   6,\n",
      "          5,   3])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 100\n",
    "dataloader = DataLoader(toxic_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for data_dict in dataloader:\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    print(data_batch)\n",
    "    break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNet(nn.Module):\n",
    "    def __init__(self, max_len, num_labels, vocab_size, embedding_size, hidden_dim, output_size):\n",
    "        super(FFNet, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.linear1 = nn.Linear(embedding_size*max_len, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # We want to flatten the inputs so that we get the matrix of shape.\n",
    "        # batch_size x no. of tokens in each input * embedding_size\n",
    "        batch_size, max_len = inputs.shape\n",
    "        embedded = self.embeddings(inputs).view(batch_size, -1)\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        probs = F.sigmoid(out)\n",
    "        return probs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1596 [00:00<?, ?it/s]D:\\apps\\Anaconda3\\envs\\torch-nlp\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "  7%|█████▏                                                                         | 104/1596 [00:04<00:54, 27.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2120, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████                                                                     | 204/1596 [00:07<00:49, 27.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1692, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████                                                                | 304/1596 [00:11<00:46, 27.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1528, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▉                                                           | 404/1596 [00:14<00:43, 27.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1415, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████▉                                                      | 504/1596 [00:18<00:39, 27.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1346, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████▉                                                 | 604/1596 [00:21<00:36, 27.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1298, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████▊                                            | 703/1596 [00:25<00:33, 26.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1241, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████▊                                       | 805/1596 [00:29<00:29, 27.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1203, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████▊                                  | 905/1596 [00:32<00:25, 27.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1173, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████████                             | 1003/1596 [00:36<00:22, 26.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1145, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|█████████████████████████████████████████████████████▉                        | 1104/1596 [00:39<00:18, 26.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1117, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████▊                   | 1203/1596 [00:43<00:15, 25.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1104, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████▋              | 1304/1596 [00:46<00:10, 26.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1084, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████▌         | 1404/1596 [00:50<00:07, 25.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1067, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████████▌    | 1504/1596 [00:53<00:03, 26.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1054, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1596/1596 [00:57<00:00, 27.89it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-fac112076d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "embedding_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# Hint: the CBOW model object you've created.\n",
    "model = FFNet(toxic_data.max_len, \n",
    "              len(label_column_names),\n",
    "              toxic_data.vocab_size, \n",
    "              embedding_size=embedding_size, \n",
    "              hidden_dim=hidden_size,\n",
    "             output_size=6).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#model = nn.DataParallel(model)\n",
    "\n",
    "losses = []\n",
    "num_epochs = 10\n",
    "for _e in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    nbatch = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x = batch['x'].to(device)\n",
    "        x_len = batch['x_len'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.float().data)\n",
    "        nbatch = nbatch + 1\n",
    "        if nbatch % 100 == 0:\n",
    "            print(sum(epoch_loss)/len(epoch_loss))\n",
    "        \n",
    "    losses.append(epoch_loss/nbatch)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    # Vectorize and Pad.\n",
    "    vectorized_sent = toxic_data.vectorize(word_tokenize(text))\n",
    "    pad_dim = (0, toxic_data.max_len - len(vectorized_sent))\n",
    "    vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "    # Forward Propagation.\n",
    "    # Unsqueeze because model is expecting `batch_size` x `sequence_len` shape.\n",
    "    outputs = model(vectorized_sent.unsqueeze(0).to(device))\n",
    "    # To get the boolean output, we check if outputs are > 0.5\n",
    "    return [int(l > 0.5) for l in outputs.squeeze()]\n",
    "    # What happens if you use torch.max instead? =)\n",
    "    ##return label_column_names[int(torch.max(outputs, dim=1).indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I will kill you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_column_names)\n",
    "predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
